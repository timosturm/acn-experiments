{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-04T10:02:26.774490Z",
     "start_time": "2024-10-04T10:02:26.759871Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T10:02:34.248749Z",
     "start_time": "2024-10-04T10:02:26.804773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from gymportal.data.ev_generators import get_standard_generator, RealWorldGenerator\n",
    "from acnportal.acnsim import Linear2StageBattery\n",
    "from gymportal.data.battery_generators import CustomizableBatteryGenerator\n",
    "from gymportal.sim import get_charging_network, Recomputer, EvaluationSimulator, SimGenerator\n",
    "from icecream import ic\n",
    "\n",
    "# charging_network = get_charging_network('simple_acn', basic_evse=True, voltage=208,\n",
    "#                                         network_kwargs={\n",
    "#                                             'station_ids': ['CA-504', 'CA-503', 'CA-502', 'CA-501'],\n",
    "#                                             'station_ids': ['CA-501'],\n",
    "# \"aggregate_cap\": 32 * 208 / 1000})\n",
    "\n",
    "charging_network = get_charging_network('caltech', basic_evse=True, voltage=208,\n",
    "                                        network_kwargs={\"transformer_cap\": 40 * 32 * 208 / 1000})\n",
    "\n",
    "battery_generator = CustomizableBatteryGenerator(voltage=208,\n",
    "                                                 period=1,\n",
    "                                                 battery_types=[\n",
    "                                                     Linear2StageBattery],\n",
    "                                                 max_power_function='normal')\n",
    "\n",
    "ev_generator = RealWorldGenerator(battery_generator=battery_generator, site='caltech', period=1)\n",
    "# ev_generator = get_standard_generator('caltech', battery_generator, seed=42)\n",
    "\n",
    "train_generator = SimGenerator(\n",
    "    charging_network=charging_network,\n",
    "    simulation_days=7,\n",
    "    n_intervals=46,\n",
    "    start_date=datetime(2019, 1, 1),\n",
    "    ev_generator=ev_generator,\n",
    "    recomputer=Recomputer(recompute_interval=10, sparse=True),\n",
    "    sim_class=EvaluationSimulator,\n",
    ")\n",
    "\n",
    "ic(train_generator.end_date + timedelta(days=1))\n",
    "\n",
    "eval_generator = SimGenerator(\n",
    "    charging_network=charging_network,\n",
    "    simulation_days=14,\n",
    "    n_intervals=1,\n",
    "    start_date=train_generator.end_date + timedelta(days=1),\n",
    "    ev_generator=ev_generator,\n",
    "    recomputer=Recomputer(recompute_interval=10, sparse=True),\n",
    "    sim_class=EvaluationSimulator,\n",
    ")\n",
    "\n",
    "ic(eval_generator.end_date + timedelta(days=1))\n",
    "\n",
    "validation_generator = SimGenerator(\n",
    "    charging_network=charging_network,\n",
    "    simulation_days=14,\n",
    "    n_intervals=1,\n",
    "    start_date=eval_generator.end_date + timedelta(days=1),\n",
    "    ev_generator=ev_generator,\n",
    "    recomputer=Recomputer(recompute_interval=10, sparse=True),\n",
    "    sim_class=EvaluationSimulator,\n",
    ")\n",
    "\n",
    "ic(validation_generator.end_date + timedelta(days=1))\n",
    "pass"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| train_generator.end_date + timedelta(days=1): datetime.datetime(2019, 11, 20, 0, 0)\n",
      "ic| eval_generator.end_date + timedelta(days=1): datetime.datetime(2019, 12, 5, 0, 0)\n",
      "ic| validation_generator.end_date + timedelta(days=1): datetime.datetime(2019, 12, 20, 0, 0)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from gymportal.environment import *\n",
    "\n",
    "observation_objects = [\n",
    "    charging_rates_observation_normalized(),\n",
    "    percentage_of_magnitude_observation(),\n",
    "    diff_pilots_charging_rates_observation_normalized(),\n",
    "    cyclical_minute_observation(),\n",
    "    cyclical_day_observation(),\n",
    "    cyclical_month_observation(),\n",
    "    cyclical_minute_observation_stay(),\n",
    "    energy_delivered_observation_normalized(),\n",
    "    num_active_stations_observation_normalized(),\n",
    "    pilot_signals_observation_normalized(),\n",
    "    cyclical_minute_observation_arrival(),\n",
    "    cyclical_day_observation_arrival(),\n",
    "    cyclical_month_observation_arrival(),\n",
    "]\n",
    "\n",
    "reward_objects = [\n",
    "    # current_constraint_violation(),\n",
    "    # soft_charging_reward(),\n",
    "    constraint_charging_reward(),\n",
    "    # pilot_charging_rate_difference_penalty(),\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T10:02:34.270170Z",
     "start_time": "2024-10-04T10:02:34.258245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from extra import unplug_penalty, ranking_schedule\n",
    "\n",
    "train_config = {\"observation_objects\": observation_objects, \"action_object\": ranking_schedule(),\n",
    "                \"reward_objects\": [unplug_penalty(), constraint_charging_reward()],\n",
    "                \"simgenerator\": train_generator,\n",
    "                \"meet_constraints\": False}\n",
    "\n",
    "eval_config = train_config | {'simgenerator': eval_generator}\n",
    "validation_config = train_config | {'simgenerator': validation_generator}"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T10:02:34.287968Z",
     "start_time": "2024-10-04T10:02:34.272077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# import tyro\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T10:02:34.296458Z",
     "start_time": "2024-10-04T10:02:34.289258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_env(env_id, idx, capture_video, run_name, gamma):\n",
    "    def thunk():\n",
    "        # if capture_video and idx == 0:\n",
    "        #     env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "        #     env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        # else:\n",
    "        #     env = gym.make(env_id)\n",
    "        env = SingleAgentSimEnv(config=train_config)\n",
    "            \n",
    "        env = gym.wrappers.FlattenObservation(env)  # deal with dm_control's Dict observation space\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        env = gym.wrappers.ClipAction(env)\n",
    "        env = gym.wrappers.NormalizeObservation(env)\n",
    "        env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))\n",
    "        env = gym.wrappers.NormalizeReward(env, gamma=gamma)\n",
    "        env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))\n",
    "        \n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T10:02:34.309109Z",
     "start_time": "2024-10-04T10:02:34.298390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        self.actor_mean = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, np.prod(envs.single_action_space.shape)), std=0.01),\n",
    "        )\n",
    "        self.actor_logstd = nn.Parameter(torch.zeros(1, np.prod(envs.single_action_space.shape)))\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        action_mean = self.actor_mean(x)\n",
    "        action_logstd = self.actor_logstd.expand_as(action_mean)\n",
    "        action_std = torch.exp(action_logstd)\n",
    "        probs = Normal(action_mean, action_std)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(x)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T10:02:34.323743Z",
     "start_time": "2024-10-04T10:02:34.310462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    # exp_name: str = os.path.basename(__file__)[: -len(\".py\")]\n",
    "    exp_name: str = None\n",
    "    \"\"\"the name of this experiment\"\"\"\n",
    "    seed: int = 1\n",
    "    \"\"\"seed of the experiment\"\"\"\n",
    "    torch_deterministic: bool = True\n",
    "    \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n",
    "    cuda: bool = True\n",
    "    \"\"\"if toggled, cuda will be enabled by default\"\"\"\n",
    "    track: bool = False\n",
    "    \"\"\"if toggled, this experiment will be tracked with Weights and Biases\"\"\"\n",
    "    wandb_project_name: str = \"cleanRL\"\n",
    "    \"\"\"the wandb's project name\"\"\"\n",
    "    wandb_entity: str = None\n",
    "    \"\"\"the entity (team) of wandb's project\"\"\"\n",
    "    capture_video: bool = False\n",
    "    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n",
    "    save_model: bool = False\n",
    "    \"\"\"whether to save model into the `runs/{run_name}` folder\"\"\"\n",
    "    upload_model: bool = False\n",
    "    \"\"\"whether to upload the saved model to huggingface\"\"\"\n",
    "    hf_entity: str = \"\"\n",
    "    \"\"\"the user or org name of the model repository from the Hugging Face Hub\"\"\"\n",
    "\n",
    "    # Algorithm specific arguments\n",
    "    env_id: str = \"HalfCheetah-v4\"\n",
    "    \"\"\"the id of the environment\"\"\"\n",
    "    total_timesteps: int = 1000000\n",
    "    \"\"\"total timesteps of the experiments\"\"\"\n",
    "    learning_rate: float = 3e-4\n",
    "    \"\"\"the learning rate of the optimizer\"\"\"\n",
    "    num_envs: int = 1\n",
    "    \"\"\"the number of parallel game environments\"\"\"\n",
    "    num_steps: int = 2048\n",
    "    \"\"\"the number of steps to run in each environment per policy rollout\"\"\"\n",
    "    anneal_lr: bool = True\n",
    "    \"\"\"Toggle learning rate annealing for policy and value networks\"\"\"\n",
    "    gamma: float = 0.99\n",
    "    \"\"\"the discount factor gamma\"\"\"\n",
    "    gae_lambda: float = 0.95\n",
    "    \"\"\"the lambda for the general advantage estimation\"\"\"\n",
    "    num_minibatches: int = 32\n",
    "    \"\"\"the number of mini-batches\"\"\"\n",
    "    update_epochs: int = 10\n",
    "    \"\"\"the K epochs to update the policy\"\"\"\n",
    "    norm_adv: bool = True\n",
    "    \"\"\"Toggles advantages normalization\"\"\"\n",
    "    clip_coef: float = 0.2\n",
    "    \"\"\"the surrogate clipping coefficient\"\"\"\n",
    "    clip_vloss: bool = True\n",
    "    \"\"\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\"\"\"\n",
    "    ent_coef: float = 0.0\n",
    "    \"\"\"coefficient of the entropy\"\"\"\n",
    "    vf_coef: float = 0.5\n",
    "    \"\"\"coefficient of the value function\"\"\"\n",
    "    max_grad_norm: float = 0.5\n",
    "    \"\"\"the maximum norm for the gradient clipping\"\"\"\n",
    "    target_kl: float = None\n",
    "    \"\"\"the target KL divergence threshold\"\"\"\n",
    "\n",
    "    # to be filled in runtime\n",
    "    batch_size: int = 0\n",
    "    \"\"\"the batch size (computed in runtime)\"\"\"\n",
    "    minibatch_size: int = 0\n",
    "    \"\"\"the mini-batch size (computed in runtime)\"\"\"\n",
    "    num_iterations: int = 0\n",
    "    \"\"\"the number of iterations (computed in runtime)\"\"\""
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T10:02:34.337977Z",
     "start_time": "2024-10-04T10:02:34.324965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "args = Args(\n",
    "    exp_name=\"Test experiment\",\n",
    "    seed = 42,\n",
    "    torch_deterministic=True,\n",
    "    cuda=True,\n",
    "    track=False,\n",
    "    save_model=False,\n",
    "    env_id=\"HalfCheetah-v4\",\n",
    "    total_timesteps=10000,\n",
    "    learning_rate=3e-4,\n",
    "    num_envs=1,\n",
    "    num_steps=1000,\n",
    "    anneal_lr=True,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    num_minibatches=32,\n",
    "    update_epochs=10,\n",
    "    norm_adv=True,\n",
    "    clip_coef=0.2,\n",
    "    clip_vloss=True,\n",
    "    ent_coef=0.0,\n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.5,\n",
    "    target_kl=None,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T10:02:34.352296Z",
     "start_time": "2024-10-04T10:02:34.339107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# args = tyro.cli(Args)\n",
    "args.batch_size = int(args.num_envs * args.num_steps)\n",
    "args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "args.num_iterations = args.total_timesteps // args.batch_size\n",
    "run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "\n",
    "if args.track:\n",
    "    import wandb\n",
    "\n",
    "    wandb.init(\n",
    "        project=args.wandb_project_name,\n",
    "        entity=args.wandb_entity,\n",
    "        sync_tensorboard=True,\n",
    "        config=vars(args),\n",
    "        name=run_name,\n",
    "        monitor_gym=True,\n",
    "        save_code=True,\n",
    "    )\n",
    "    \n",
    "writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "writer.add_text(\n",
    "    \"hyperparameters\",\n",
    "    \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T10:09:01.938351Z",
     "start_time": "2024-10-04T10:02:34.354962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TRY NOT TO MODIFY: seeding\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "\n",
    "# env setup\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(args.env_id, i, args.capture_video, run_name, args.gamma) for i in range(args.num_envs)]\n",
    ")\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Box), \"only continuous action space is supported\"\n",
    "\n",
    "agent = Agent(envs).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n",
    "\n",
    "# ALGO Logic: Storage setup\n",
    "obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n",
    "actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n",
    "logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "values = torch.zeros((args.num_steps, args.num_envs)).to(device)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-04T10:09:01.939562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TRY NOT TO MODIFY: start the game\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "next_obs, _ = envs.reset(seed=args.seed)\n",
    "next_obs = torch.Tensor(next_obs).to(device)\n",
    "next_done = torch.zeros(args.num_envs).to(device)\n",
    "\n",
    "for iteration in range(1, args.num_iterations + 1):\n",
    "    # Annealing the rate if instructed to do so.\n",
    "    if args.anneal_lr:\n",
    "        frac = 1.0 - (iteration - 1.0) / args.num_iterations\n",
    "        lrnow = frac * args.learning_rate\n",
    "        optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "    for step in range(0, args.num_steps):\n",
    "        global_step += args.num_envs\n",
    "        obs[step] = next_obs\n",
    "        dones[step] = next_done\n",
    "\n",
    "        # ALGO LOGIC: action logic\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "            values[step] = value.flatten()\n",
    "        actions[step] = action\n",
    "        logprobs[step] = logprob\n",
    "\n",
    "        # TRY NOT TO MODIFY: execute the game and log data.\n",
    "        next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())\n",
    "        next_done = np.logical_or(terminations, truncations)\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)\n",
    "\n",
    "        if \"final_info\" in infos:\n",
    "            for info in infos[\"final_info\"]:\n",
    "                if info and \"episode\" in info:\n",
    "                    print(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\n",
    "                    writer.add_scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
    "                    writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "\n",
    "    # bootstrap value if not done\n",
    "    with torch.no_grad():\n",
    "        next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "        advantages = torch.zeros_like(rewards).to(device)\n",
    "        lastgaelam = 0\n",
    "        for t in reversed(range(args.num_steps)):\n",
    "            if t == args.num_steps - 1:\n",
    "                nextnonterminal = 1.0 - next_done\n",
    "                nextvalues = next_value\n",
    "            else:\n",
    "                nextnonterminal = 1.0 - dones[t + 1]\n",
    "                nextvalues = values[t + 1]\n",
    "            delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n",
    "            advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
    "        returns = advantages + values\n",
    "\n",
    "    # flatten the batch\n",
    "    b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_logprobs = logprobs.reshape(-1)\n",
    "    b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "    b_advantages = advantages.reshape(-1)\n",
    "    b_returns = returns.reshape(-1)\n",
    "    b_values = values.reshape(-1)\n",
    "\n",
    "    # Optimizing the policy and value network\n",
    "    b_inds = np.arange(args.batch_size)\n",
    "    clipfracs = []\n",
    "    for epoch in range(args.update_epochs):\n",
    "        np.random.shuffle(b_inds)\n",
    "        for start in range(0, args.batch_size, args.minibatch_size):\n",
    "            end = start + args.minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "\n",
    "            _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])\n",
    "            logratio = newlogprob - b_logprobs[mb_inds]\n",
    "            ratio = logratio.exp()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                old_approx_kl = (-logratio).mean()\n",
    "                approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
    "\n",
    "            mb_advantages = b_advantages[mb_inds]\n",
    "            if args.norm_adv:\n",
    "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "            # Policy loss\n",
    "            pg_loss1 = -mb_advantages * ratio\n",
    "            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            # Value loss\n",
    "            newvalue = newvalue.view(-1)\n",
    "            if args.clip_vloss:\n",
    "                v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                    newvalue - b_values[mb_inds],\n",
    "                    -args.clip_coef,\n",
    "                    args.clip_coef,\n",
    "                )\n",
    "                v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                v_loss = 0.5 * v_loss_max.mean()\n",
    "            else:\n",
    "                v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "        if args.target_kl is not None and approx_kl > args.target_kl:\n",
    "            break\n",
    "\n",
    "    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "    var_y = np.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "    # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "    writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "    writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "    writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
    "    writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
    "    writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
    "    writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "    print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "    writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "\n",
    "if args.save_model:\n",
    "    model_path = f\"runs/{run_name}/{args.exp_name}.cleanrl_model\"\n",
    "    torch.save(agent.state_dict(), model_path)\n",
    "    print(f\"model saved to {model_path}\")\n",
    "    from cleanrl_utils.evals.ppo_eval import evaluate\n",
    "\n",
    "    episodic_returns = evaluate(\n",
    "        model_path,\n",
    "        make_env,\n",
    "        args.env_id,\n",
    "        eval_episodes=10,\n",
    "        run_name=f\"{run_name}-eval\",\n",
    "        Model=Agent,\n",
    "        device=device,\n",
    "        gamma=args.gamma,\n",
    "    )\n",
    "    for idx, episodic_return in enumerate(episodic_returns):\n",
    "        writer.add_scalar(\"eval/episodic_return\", episodic_return, idx)\n",
    "\n",
    "    if args.upload_model:\n",
    "        from cleanrl_utils.huggingface import push_to_hub\n",
    "\n",
    "        repo_name = f\"{args.env_id}-{args.exp_name}-seed{args.seed}\"\n",
    "        repo_id = f\"{args.hf_entity}/{repo_name}\" if args.hf_entity else repo_name\n",
    "        push_to_hub(args, episodic_returns, repo_id, \"PPO\", f\"runs/{run_name}\", f\"videos/{run_name}-eval\")\n",
    "\n",
    "envs.close()\n",
    "writer.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# from gymportal.auxilliaries.callbacks import MetricsCallback\n",
    "# from ray.rllib.algorithms.ppo import PPOConfig\n",
    "# \n",
    "# # lr_schedule = [[0, 5e-5], [int(1e20), 5e-ranking]]\n",
    "# \n",
    "# config = (\n",
    "#     PPOConfig()\n",
    "#     .environment('single_agent_env', env_config=train_config)\n",
    "#     .rollouts(num_rollout_workers=1)\n",
    "#     .framework(\"tf2\")\n",
    "#     .evaluation(evaluation_num_workers=1, evaluation_interval=3)\n",
    "#     .training(use_kl_loss=False, kl_coeff=0.0)\n",
    "#     # .training(lr_schedule=lr_schedule)\n",
    "#     # .exploration(explore=False, exploration_config={\"type\": \"StochasticSampling\"})\n",
    "#     .callbacks(MetricsCallback)\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# algo = config.build()",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# from gymportal.evaluation import ACNSchedule, RllibSchedule\n",
    "# from acnportal.algorithms import UncontrolledCharging, SortedSchedulingAlgo, last_come_first_served, \\\n",
    "#     first_come_first_served\n",
    "# \n",
    "# models = {\n",
    "#     \"PPO\": RllibSchedule(algo),\n",
    "#     \"FCFS\": ACNSchedule(SortedSchedulingAlgo(first_come_first_served)),\n",
    "#     \"LCFS\": ACNSchedule(SortedSchedulingAlgo(last_come_first_served)),\n",
    "#     \"Uncontrolled\": ACNSchedule(UncontrolledCharging()),\n",
    "# }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# from cleanRL.run_simulation import run_simulations, metrics\n",
    "# \n",
    "# df = run_simulations(models, metrics=metrics, config=validation_config)\n",
    "# df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# df.to_csv(\"before.csv\")\n",
    "# ax = df.plot.bar()\n",
    "# fig = ax.get_figure()\n",
    "# fig.savefig(\"before.png\", dpi=600)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# res = []\n",
    "# \n",
    "# for i in tqdm(range(12), desc='training iteration'):\n",
    "#     res.append(algo.train())\n",
    "# \n",
    "#     algo.save(checkpoint_dir=f\"{algo}_check\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# from gymportal.plotting.plotting import plot_training\n",
    "# \n",
    "# _ = plot_training(res).savefig(f\"{algo}\", dpi=600)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# plain     31:28\n",
    "# cython    33:21\n",
    "# v2        31:58"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# from ray.rllib.algorithms import Algorithm\n",
    "# #algo = Algorithm.from_checkpoint(checkpoint_dir + \"/checkpoint_000001\")\n",
    "# \n",
    "# algo = Algorithm.from_checkpoint(\"PPO_check\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "# algo.evaluate()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "eval_generator = SimGenerator(\n",
    "    charging_network=charging_network,\n",
    "    simulation_days=1,\n",
    "    n_intervals=1,\n",
    "    start_date=datetime(2019, 9, 23),\n",
    "    ev_generator=ev_generator,\n",
    "    recomputer=Recomputer(recompute_interval=10, sparse=True),\n",
    "    sim_class=EvaluationSimulator,\n",
    ")\n",
    "\n",
    "evaluation_config = train_config | {\"simgenerator\": eval_generator}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from gymportal.evaluation import evaluate_model\n",
    "\n",
    "eval_sim = evaluate_model(RllibSchedule(algo), env_type=SingleAgentSimEnv, env_config=evaluation_config)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from gymportal.plotting.plotting import plot_sim_evaluation\n",
    "\n",
    "_ = plot_sim_evaluation(eval_sim)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "df = run_simulations(models, metrics=metrics, config=validation_config)\n",
    "df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "df.to_csv(\"after.csv\")\n",
    "ax = df.plot.bar()\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(\"after.png\", dpi=600)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
