{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from acn_experiments.run_simulation import run_simulations, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymportal.data.ev_generators import get_standard_generator, RealWorldGenerator\n",
    "from acnportal.acnsim import Linear2StageBattery\n",
    "from gymportal.data.battery_generators import CustomizableBatteryGenerator\n",
    "from gymportal.sim import get_charging_network, Recomputer, EvaluationSimulator, SimGenerator\n",
    "from icecream import ic\n",
    "\n",
    "import pytz\n",
    "timezone = pytz.timezone(\"America/Los_Angeles\")\n",
    "\n",
    "charging_network = get_charging_network('simple_acn', basic_evse=True, voltage=208,\n",
    "                                        network_kwargs={\n",
    "                                            'station_ids': ['CA-504', 'CA-503', 'CA-502', 'CA-501'],\n",
    "                                             #'station_ids': ['CA-501'],\n",
    "\"aggregate_cap\": 32 * 208 / 1000})\n",
    "\n",
    "# charging_network = get_charging_network('caltech', basic_evse=True, voltage=208,\n",
    "#                                         network_kwargs={\"transformer_cap\": 40 * 32 * 208 / 1000})\n",
    "\n",
    "battery_generator = CustomizableBatteryGenerator(voltage=208,\n",
    "                                                 period=1,\n",
    "                                                 battery_types=[\n",
    "                                                     Linear2StageBattery],\n",
    "                                                 max_power_function='normal')\n",
    "\n",
    "ev_generator = RealWorldGenerator(battery_generator=battery_generator, site='caltech', period=1)\n",
    "# ev_generator = get_standard_generator('caltech', battery_generator, seed=42)\n",
    "\n",
    "train_generator = SimGenerator(\n",
    "    charging_network=charging_network,\n",
    "    simulation_days=7,\n",
    "    n_intervals=46,\n",
    "    start_date=timezone.localize(datetime(2019, 1, 1)),\n",
    "    ev_generator=ev_generator,\n",
    "    recomputer=Recomputer(recompute_interval=10, sparse=True),\n",
    "    sim_class=EvaluationSimulator,\n",
    ")\n",
    "\n",
    "ic(train_generator.end_date + timedelta(days=1))\n",
    "\n",
    "eval_generator = SimGenerator(\n",
    "    charging_network=charging_network,\n",
    "    simulation_days=7,\n",
    "    n_intervals=1,\n",
    "    start_date=train_generator.end_date + timedelta(days=1),\n",
    "    ev_generator=ev_generator,\n",
    "    recomputer=Recomputer(recompute_interval=10, sparse=True),\n",
    "    sim_class=EvaluationSimulator,\n",
    ")\n",
    "\n",
    "ic(eval_generator.end_date + timedelta(days=1))\n",
    "\n",
    "validation_generator = SimGenerator(\n",
    "    charging_network=charging_network,\n",
    "    simulation_days=14,\n",
    "    n_intervals=1,\n",
    "    start_date=eval_generator.end_date + timedelta(days=1),\n",
    "    ev_generator=ev_generator,\n",
    "    recomputer=Recomputer(recompute_interval=10, sparse=True),\n",
    "    sim_class=EvaluationSimulator,\n",
    ")\n",
    "\n",
    "ic(validation_generator.end_date + timedelta(days=1))\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymportal.environment import *\n",
    "from extra import unplug_penalty, ranking_schedule\n",
    "\n",
    "\n",
    "observation_objects = [\n",
    "    charging_rates_observation_normalized(),\n",
    "    percentage_of_magnitude_observation(),\n",
    "    diff_pilots_charging_rates_observation_normalized(),\n",
    "    cyclical_minute_observation(),\n",
    "    cyclical_day_observation(),\n",
    "    cyclical_month_observation(),\n",
    "    cyclical_minute_observation_stay(),\n",
    "    energy_delivered_observation_normalized(),\n",
    "    num_active_stations_observation_normalized(),\n",
    "    pilot_signals_observation_normalized(),\n",
    "    cyclical_minute_observation_arrival(),\n",
    "    cyclical_day_observation_arrival(),\n",
    "    cyclical_month_observation_arrival(),\n",
    "]\n",
    "\n",
    "reward_objects = [\n",
    "    # current_constraint_violation(),\n",
    "    # soft_charging_reward(),\n",
    "    constraint_charging_reward(),\n",
    "    unplug_penalty(),\n",
    "    # pilot_charging_rate_difference_penalty(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_config = {\"observation_objects\": observation_objects, \"action_object\": ranking_schedule(),\n",
    "                \"reward_objects\": reward_objects,\n",
    "                \"simgenerator\": train_generator,\n",
    "                \"meet_constraints\": False}\n",
    "\n",
    "eval_config = train_config | {'simgenerator': eval_generator}\n",
    "validation_config = train_config | {'simgenerator': validation_generator}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim),\n",
    "            nn.Softmax(dim=-1)  # For action probabilities\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)  # Single output for value\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, obs_dim, action_dim, lr=1e-3, gamma=0.99, clip_epsilon=0.2):\n",
    "        super(PPOAgent, self).__init__()\n",
    "        self.policy_net = PolicyNetwork(obs_dim, action_dim)\n",
    "        self.value_net = ValueNetwork(obs_dim)\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.optimizer = None\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return self.optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        obs, actions, rewards, next_obs, dones = batch\n",
    "\n",
    "        # Compute advantages and returns\n",
    "        # (You need to implement this part based on your specific method)\n",
    "\n",
    "        # Update policy\n",
    "        old_log_probs = self.policy_net(obs).gather(1, actions.unsqueeze(-1)).log()\n",
    "        # Calculate ratios, clipped loss, etc.\n",
    "\n",
    "        # Update value function\n",
    "        value_loss = nn.MSELoss()(self.value_net(obs), returns)\n",
    "\n",
    "        # Total loss and backpropagation\n",
    "        loss = policy_loss + value_loss\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def collect_experience(self, env):\n",
    "        # Implement experience collection\n",
    "        # Use the environment to collect state, action, reward, etc.\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# from gymportal.auxilliaries.callbacks import MetricsCallback\n",
    "# from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "# # lr_schedule = [[0, 5e-5], [int(1e20), 5e-ranking]]\n",
    "\n",
    "# config = (\n",
    "#     PPOConfig()\n",
    "#     .environment('single_agent_env', env_config=train_config)\n",
    "#     .rollouts(num_rollout_workers=1)\n",
    "#     .framework(\"tf2\")\n",
    "#     .evaluation(evaluation_num_workers=1, evaluation_interval=3)\n",
    "#     .training(use_kl_loss=False, kl_coeff=0.0)\n",
    "#     # .training(lr_schedule=lr_schedule)\n",
    "#     # .exploration(explore=False, exploration_config={\"type\": \"StochasticSampling\"})\n",
    "#     .callbacks(MetricsCallback)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# algo = config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymportal.evaluation import ACNSchedule, RllibSchedule\n",
    "from acnportal.algorithms import UncontrolledCharging, SortedSchedulingAlgo, last_come_first_served, \\\n",
    "    first_come_first_served\n",
    "\n",
    "models = {\n",
    "    #\"PPO\": RllibSchedule(algo),\n",
    "    \"FCFS\": ACNSchedule(SortedSchedulingAlgo(first_come_first_served)),\n",
    "    \"LCFS\": ACNSchedule(SortedSchedulingAlgo(last_come_first_served)),\n",
    "    \"Uncontrolled\": ACNSchedule(UncontrolledCharging()),\n",
    "}\n",
    "\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = run_simulations(models, metrics=metrics, config=eval_config, seed=42)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"before.csv\")\n",
    "ax = df.plot.bar()\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(\"before.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = []\n",
    "\n",
    "# for i in tqdm(range(12), desc='training iteration'):\n",
    "#     res.append(algo.train())\n",
    "\n",
    "#     algo.save(checkpoint_dir=f\"{algo}_check\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gymportal.plotting.plotting import plot_training\n",
    "\n",
    "# _ = plot_training(res).savefig(f\"{algo}\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algo.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_generator = SimGenerator(\n",
    "    charging_network=charging_network,\n",
    "    simulation_days=1,\n",
    "    n_intervals=1,\n",
    "    start_date=datetime(2019, 9, 23),\n",
    "    ev_generator=ev_generator,\n",
    "    recomputer=Recomputer(recompute_interval=10, sparse=True),\n",
    "    sim_class=EvaluationSimulator,\n",
    ")\n",
    "\n",
    "evaluation_config = train_config | {\"simgenerator\": eval_generator}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymportal.evaluation import evaluate_model\n",
    "\n",
    "eval_sim = evaluate_model(RllibSchedule(algo), env_type=SingleAgentSimEnv, env_config=evaluation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymportal.plotting.plotting import plot_sim_evaluation\n",
    "\n",
    "_ = plot_sim_evaluation(eval_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = run_simulations(models, metrics=metrics, config=eval_config, seed=42)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"after.csv\")\n",
    "ax = df.plot.bar()\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(\"after.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
